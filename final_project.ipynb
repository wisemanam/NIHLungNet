{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-kkfDuwmS0_Lic6LbeZwhpSvhqVPlXFN",
      "authorship_tag": "ABX9TyPaXtpKs+kpy8i/sB3VYec0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wisemanam/NIHLungNet/blob/main/final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjPIzBC2tniA"
      },
      "source": [
        "# Detecting Lung Disease Using CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIKhOrDUtsSO",
        "outputId": "e684fa3c-af92-437e-e335-864dc1940e6e"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.applications import DenseNet121\n",
        "from keras.applications.densenet import preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.metrics import categorical_accuracy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swo4fAIztwPt"
      },
      "source": [
        "## Loading in the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjNPbqwit0v5"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/data/xray_data/'\n",
        "train_path = data_path + 'train/'\n",
        "test_path = data_path + 'test/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bog1YqrBuLpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d10040-582f-4636-a113-2e203a10f5d5"
      },
      "source": [
        "df = pd.read_csv(data_path + \"Data_Entry_2017.csv\")\n",
        "df[\"Finding Labels\"]=df[\"Finding Labels\"].apply(lambda x:x.split(\"|\"))\n",
        "classes = [\"No Finding\", \n",
        "           \"Atelectasis\", \n",
        "           \"Consolidation\", \n",
        "           \"Infiltration\",\n",
        "           \"Pneumothorax\",\n",
        "           \"Edema\",\n",
        "           \"Emphysema\",\n",
        "           \"Fibrosis\",\n",
        "           \"Effusion\",\n",
        "           \"Pneumonia\",\n",
        "           \"Pleural_Thickening\",\n",
        "           \"Cardiomegaly\",\n",
        "           \"Mass\",\n",
        "           \"Nodule\",\n",
        "           \"Hernia\"]\n",
        "\n",
        "datagen=ImageDataGenerator(rescale=1./255.)\n",
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "# using 15000/86524 training images\n",
        "train_it=datagen.flow_from_dataframe(dataframe=df, \n",
        "                                            directory=train_path, \n",
        "                                            x_col=\"Image Index\", \n",
        "                                            y_col=\"Finding Labels\", \n",
        "                                            batch_size=32, \n",
        "                                            seed=42, \n",
        "                                            shuffle=True, \n",
        "                                            class_mode=\"categorical\",\n",
        "                                            classes=classes,\n",
        "                                            target_size=(1024,1024))\n",
        "\n",
        "# using 2000/~20000 testing images\n",
        "test_it=test_datagen.flow_from_dataframe(dataframe=df,\n",
        "                                                directory=test_path,\n",
        "                                                x_col=\"Image Index\",\n",
        "                                                y_col=\"Finding Labels\",\n",
        "                                                batch_size=214,\n",
        "                                                seed=42,\n",
        "                                                shuffle=False,\n",
        "                                                class_mode=\"categorical\",\n",
        "                                                classes=classes,\n",
        "                                                target_size=(1024,1024))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 89060 invalid image filename(s) in x_col=\"Image Index\". These filename(s) will be ignored.\n",
            "  .format(n_invalid, x_col)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 23060 validated image filenames belonging to 15 classes.\n",
            "Found 3210 validated image filenames belonging to 15 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 108910 invalid image filename(s) in x_col=\"Image Index\". These filename(s) will be ignored.\n",
            "  .format(n_invalid, x_col)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-2yDhxOuYPx"
      },
      "source": [
        "## Creating the Model\n",
        "Next, I am going to create the model that will train on the Chest X-Rays. I am going to start with a pre-trained network and see if that helps training go more smoothly. I am using categorical crossentropy as my loss function to allow for multiple labels being present. I also use sigmoid activation to accommodate multiple labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igMp9o_tuZoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b1b9ad-7a01-466c-edf2-fd94c201ab6e"
      },
      "source": [
        "from tensorflow_addons.metrics import HammingLoss\n",
        "# load model without classifier layers\n",
        "model = DenseNet121(include_top=False, input_shape=(1024, 1024, 3), weights='imagenet', pooling='max')\n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = Flatten()(model.layers[-1].output)\n",
        "class1 = Dense(1024, activation='sigmoid')(flat1)\n",
        "output = Dense(15, activation='sigmoid')(class1)\n",
        "\n",
        "# define new model\n",
        "model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "for layer in model.layers[:-4]:\n",
        "    layer.trainable=False\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.00001)\n",
        "model.compile(loss=[tfa.losses.SigmoidFocalCrossEntropy(gamma=4.0)], metrics=[HammingLoss(mode='multiclass', threshold=0.7)], optimizer=optimizer)\n",
        "history1 = model.fit(train_it, batch_size=256, epochs=10, validation_data=test_it)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "721/721 [==============================] - 10625s 15s/step - loss: 0.1401 - hamming_loss: 0.9985 - val_loss: 0.1266 - val_hamming_loss: 1.0000\n",
            "Epoch 2/10\n",
            "721/721 [==============================] - 997s 1s/step - loss: 0.0837 - hamming_loss: 1.0000 - val_loss: 0.1238 - val_hamming_loss: 1.0000\n",
            "Epoch 3/10\n",
            "721/721 [==============================] - 988s 1s/step - loss: 0.0798 - hamming_loss: 1.0000 - val_loss: 0.1213 - val_hamming_loss: 1.0000\n",
            "Epoch 4/10\n",
            "721/721 [==============================] - 984s 1s/step - loss: 0.0775 - hamming_loss: 1.0000 - val_loss: 0.1178 - val_hamming_loss: 1.0000\n",
            "Epoch 5/10\n",
            "721/721 [==============================] - 986s 1s/step - loss: 0.0765 - hamming_loss: 1.0000 - val_loss: 0.1177 - val_hamming_loss: 1.0000\n",
            "Epoch 6/10\n",
            "721/721 [==============================] - 981s 1s/step - loss: 0.0765 - hamming_loss: 1.0000 - val_loss: 0.1184 - val_hamming_loss: 1.0000\n",
            "Epoch 7/10\n",
            "721/721 [==============================] - 973s 1s/step - loss: 0.0755 - hamming_loss: 1.0000 - val_loss: 0.1161 - val_hamming_loss: 1.0000\n",
            "Epoch 8/10\n",
            "721/721 [==============================] - 980s 1s/step - loss: 0.0741 - hamming_loss: 1.0000 - val_loss: 0.1161 - val_hamming_loss: 1.0000\n",
            "Epoch 9/10\n",
            "721/721 [==============================] - 980s 1s/step - loss: 0.0752 - hamming_loss: 1.0000 - val_loss: 0.1164 - val_hamming_loss: 1.0000\n",
            "Epoch 10/10\n",
            "721/721 [==============================] - 985s 1s/step - loss: 0.0743 - hamming_loss: 1.0000 - val_loss: 0.1155 - val_hamming_loss: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD1p-iNWEsD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "c0444d40-e21f-4bdc-f1ad-5d37c315a5fe"
      },
      "source": [
        "plt.plot( history1.epoch,history1.history['loss'], label = 'loss' )\n",
        "plt.plot( history1.epoch,history1.history['val_loss'], label = 'val_loss' )\n",
        "plt.legend()\n",
        "\n",
        "plt.plot( history1.epoch,history1.history['hamming_loss'], label = 'hamming_loss' )\n",
        "plt.plot( history1.epoch,history1.history['val_hamming_loss'], label = 'val_hamming_loss' )\n",
        "plt.legend()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e41806070a8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mhistory1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hamming_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hamming_loss'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mhistory1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val_accuracy'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceVzSGLNBu7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44e7c88-0e7a-4716-f6ea-4fa2f0e9f9f6"
      },
      "source": [
        "model.evaluate(test_it)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3210/3210 [==============================] - 140s 44ms/step - loss: 0.1155 - hamming_loss: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11547640711069107, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bZp4NvYkRKr"
      },
      "source": [
        "def get_batch_scores(labels, pred_labels):\n",
        "  batch_size = len(labels)\n",
        "  num_classes = len(labels[0])\n",
        "\n",
        "  accuracy_sum = 0\n",
        "  precision_sum = 0\n",
        "  recall_sum = 0\n",
        "\n",
        "  accuracy_by_class = []\n",
        "  precision_by_class = []\n",
        "  recall_by_class = []\n",
        "\n",
        "  for j in range(num_classes): # for each class\n",
        "    tp_count = 0\n",
        "    fn_count = 0\n",
        "    fp_count = 0\n",
        "    num_correct = 0\n",
        "    for i in range(batch_size): # for each image in batch\n",
        "      # counts number of correct predictions\n",
        "      if pred_labels[i][j] == labels[i][j]:\n",
        "        num_correct += 1\n",
        "\n",
        "      # counts number of true positives\n",
        "      if pred_labels[i][j] == 1 and labels[i][j] == 1:\n",
        "        tp_count += 1\n",
        "\n",
        "      # counts number of false negatives\n",
        "      elif pred_labels[i][j] == 0 and labels[i][j] == 1:\n",
        "        fn_count += 1\n",
        "      \n",
        "      # counts number of false positives\n",
        "      elif pred_labels[i][j] == 1 and labels[i][j] == 0:\n",
        "        fp_count += 1\n",
        "\n",
        "    # gets the sum of all the scores in the batch\n",
        "    accuracy_sum += num_correct/15\n",
        "    if (tp_count + fp_count) != 0:\n",
        "      precision_sum += tp_count/(tp_count + fp_count)\n",
        "    if (tp_count + fn_count) != 0:\n",
        "      recall_sum += tp_count/(tp_count + fn_count)\n",
        "\n",
        "    # averages the score across the batch by dividing by the batch size\n",
        "    accuracy_by_class.append(accuracy_sum/batch_size)\n",
        "    precision_by_class.append(precision_sum/batch_size)\n",
        "    recall_by_class.append(recall_sum/batch_size)\n",
        "\n",
        "  # returns average scores across the batch\n",
        "  return accuracy_by_class, precision_by_class, recall_by_class"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "vfkQ7yqlc0kN",
        "outputId": "5e896be6-3c4d-4f60-ce93-0e62c01086a3"
      },
      "source": [
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "\n",
        "for batch in test_it:\n",
        "  images = batch[0]\n",
        "  labels = batch[1]\n",
        "  print('Images shape:', images.shape)\n",
        "  print('Labels shape:', labels.shape)\n",
        "  # print('true_labels: \\n', labels)\n",
        "  pred = model.predict(images)\n",
        "  pred_labels = 1*(pred>0.5)\n",
        "  # print('pred_labels: \\n', pred_labels)\n",
        "\n",
        "  # gets average scores from the batch\n",
        "  batch_accuracy, batch_precision, batch_recall = get_batch_scores(labels, pred_labels) \n",
        "  accuracies.append(batch_accuracy)\n",
        "  precisions.append(batch_precision)\n",
        "  recalls.append(batch_recall)\n",
        "\n",
        "# averages the scores of all the batches\n",
        "accuracies = [sum(x) / len(x) for x in zip(*accuracies)]\n",
        "precisions = [sum(x) / len(x) for x in zip(*precisions)]\n",
        "recalls = [sum(x) / len(x) for x in zip(*recalls)]\n",
        "\n",
        "print('Accuracy: ', accuracies)\n",
        "print('Precision: ', precisions)\n",
        "print('Recall: ', recalls)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-de849fb3849c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrecalls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_it' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my5quW2uo76l"
      },
      "source": [
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}